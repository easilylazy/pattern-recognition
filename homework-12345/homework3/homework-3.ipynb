{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Homework-3: MNIST Classification with ConvNet\n",
    "\n",
    "### **Deadline: 2021.04.06 23:59:00 **\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement the forward and backward functions for ConvLayer (`layers/conv_layer.py`)\n",
    "- #### implement the forward and backward functions for PoolingLayer (`layers/pooling_layer.py`)\n",
    "- #### implement the forward and backward functions for DropoutLayer (`layers/dropout_layer.py`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [1, 28, 28])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Preprocessing\r\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\r\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\r\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\r\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\r\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\r\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Hyperparameters\n",
    "You can modify hyperparameters by yourself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 100\r\n",
    "max_epoch = 10\r\n",
    "init_std = 0.01\r\n",
    "\r\n",
    "learning_rate = 0.001\r\n",
    "weight_decay = 0.005\r\n",
    "\r\n",
    "disp_freq = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Criterion and Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\r\n",
    "from optimizer import SGD\r\n",
    "\r\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\r\n",
    "sgd = SGD(learning_rate, weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConvNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from layers import FCLayer, ReLULayer, ConvLayer, MaxPoolingLayer, ReshapeLayer\r\n",
    "\r\n",
    "convNet = Network()\r\n",
    "convNet.add(ConvLayer(1, 8, 3, 1))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(MaxPoolingLayer(2, 0))\r\n",
    "convNet.add(ConvLayer(8, 16, 3, 1))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(MaxPoolingLayer(2, 0))\r\n",
    "convNet.add(ReshapeLayer((batch_size, 16, 7, 7), (batch_size, 784)))\r\n",
    "convNet.add(FCLayer(784, 128))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(FCLayer(128, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train\r\n",
    "convNet.is_training = True\r\n",
    "convNet, conv_loss, conv_acc = train(convNet, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test\r\n",
    "convNet.is_training = False\r\n",
    "test(convNet, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_loss_and_acc({'ConvNet': [conv_loss, conv_acc]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ~~You have finished homework3, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements (4):**\n",
    "### **You need to implement the Dropout layer and train the network again.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "size=7\r\n",
    "drop_out=0.2\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from layers import DropoutLayer\r\n",
    "\r\n",
    "\r\n",
    "convNet = Network()\r\n",
    "\r\n",
    "\r\n",
    "convNet.add(ConvLayer(1, 8, 3, 1))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(MaxPoolingLayer(2, 0))\r\n",
    "convNet.add(ConvLayer(8, 16, 3, 1))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(MaxPoolingLayer(2, 0))\r\n",
    "convNet.add(ReshapeLayer((batch_size, 16, 7, 7), (batch_size, 784)))\r\n",
    "convNet.add(FCLayer(16* size* size, 128))\r\n",
    "convNet.add(DropoutLayer(p=drop_out))\r\n",
    "convNet.add(ReLULayer())\r\n",
    "convNet.add(FCLayer(128, 10))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train\r\n",
    "convNet.is_training = True\r\n",
    "convNet, conv_loss, conv_acc = train(convNet, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test\r\n",
    "convNet.is_training = False\r\n",
    "test(convNet, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_loss_and_acc({'ConvNet': [conv_loss, conv_acc]})"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "3f857618d8b6a3d270a8a45aea13fd0dc2a28a907e8d4ef70fe87ce2a92698f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}