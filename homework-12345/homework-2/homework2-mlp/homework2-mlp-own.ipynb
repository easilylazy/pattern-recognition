{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "tf.disable_eager_execution()\r\n",
    "\r\n",
    "from network import Network\r\n",
    "from solver import train, test\r\n",
    "from plot import plot_loss_and_acc"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\cascara\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def decode_image(image):\r\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\r\n",
    "    image = tf.cast(image, tf.float32)\r\n",
    "    image = tf.reshape(image, [784])\r\n",
    "    image = image / 255.0\r\n",
    "    image = image - tf.reduce_mean(image)\r\n",
    "    return image\r\n",
    "\r\n",
    "def decode_label(label):\r\n",
    "    # Encode label with one-hot encoding\r\n",
    "    return tf.one_hot(label, depth=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Data Preprocessing\r\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\r\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\r\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\r\n",
    "\r\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\r\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\r\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "batch_size = 100\r\n",
    "max_epoch = 2\r\n",
    "init_std = 0.01\r\n",
    "\r\n",
    "learning_rate_SGD = 0.001\r\n",
    "weight_decay = 0.1\r\n",
    "\r\n",
    "hidden_layer1=128\r\n",
    "hidden_layer2=64\r\n",
    "disp_freq = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from criterion import EuclideanLossLayer\r\n",
    "from optimizer import SGD\r\n",
    "\r\n",
    "criterion = EuclideanLossLayer()\r\n",
    "\r\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\r\n",
    "\r\n",
    "### TODO\r\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from layers import FCLayer, SigmoidLayer\r\n",
    "\r\n",
    "sigmoidMLP = Network()\r\n",
    "# Build MLP with FCLayer and SigmoidLayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "sigmoidMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cascara\\Desktop\\code\\2021spring\\patternRecognition\\homework-12345\\homework-2\\homework2-mlp\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch [0][2]\t Batch [0][550]\t Training Loss 11.1182\t Accuracy 0.1200\n",
      "Epoch [0][2]\t Batch [50][550]\t Training Loss 3.7186\t Accuracy 0.0759\n",
      "Epoch [0][2]\t Batch [100][550]\t Training Loss 2.5076\t Accuracy 0.0895\n",
      "Epoch [0][2]\t Batch [150][550]\t Training Loss 2.0776\t Accuracy 0.0991\n",
      "Epoch [0][2]\t Batch [200][550]\t Training Loss 1.8490\t Accuracy 0.1073\n",
      "Epoch [0][2]\t Batch [250][550]\t Training Loss 1.7059\t Accuracy 0.1157\n",
      "Epoch [0][2]\t Batch [300][550]\t Training Loss 1.6043\t Accuracy 0.1250\n",
      "Epoch [0][2]\t Batch [350][550]\t Training Loss 1.5296\t Accuracy 0.1337\n",
      "Epoch [0][2]\t Batch [400][550]\t Training Loss 1.4713\t Accuracy 0.1400\n",
      "Epoch [0][2]\t Batch [450][550]\t Training Loss 1.4231\t Accuracy 0.1464\n",
      "Epoch [0][2]\t Batch [500][550]\t Training Loss 1.3819\t Accuracy 0.1539\n",
      "\n",
      "Epoch [0]\t Average training loss 1.3477\t Average training accuracy 0.1610\n",
      "Epoch [0]\t Average validation loss 0.9897\t Average validation accuracy 0.2430\n",
      "\n",
      "Epoch [1][2]\t Batch [0][550]\t Training Loss 0.9833\t Accuracy 0.2600\n",
      "Epoch [1][2]\t Batch [50][550]\t Training Loss 0.9818\t Accuracy 0.2453\n",
      "Epoch [1][2]\t Batch [100][550]\t Training Loss 0.9740\t Accuracy 0.2534\n",
      "Epoch [1][2]\t Batch [150][550]\t Training Loss 0.9683\t Accuracy 0.2563\n",
      "Epoch [1][2]\t Batch [200][550]\t Training Loss 0.9592\t Accuracy 0.2649\n",
      "Epoch [1][2]\t Batch [250][550]\t Training Loss 0.9502\t Accuracy 0.2735\n",
      "Epoch [1][2]\t Batch [300][550]\t Training Loss 0.9409\t Accuracy 0.2820\n",
      "Epoch [1][2]\t Batch [350][550]\t Training Loss 0.9342\t Accuracy 0.2881\n",
      "Epoch [1][2]\t Batch [400][550]\t Training Loss 0.9277\t Accuracy 0.2944\n",
      "Epoch [1][2]\t Batch [450][550]\t Training Loss 0.9214\t Accuracy 0.3010\n",
      "Epoch [1][2]\t Batch [500][550]\t Training Loss 0.9147\t Accuracy 0.3081\n",
      "\n",
      "Epoch [1]\t Average training loss 0.9087\t Average training accuracy 0.3143\n",
      "Epoch [1]\t Average validation loss 0.8387\t Average validation accuracy 0.3904\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.3922.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\r\n",
    "\r\n",
    "### TODO\r\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from layers import ReLULayer\r\n",
    "\r\n",
    "reluMLP = Network()\r\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\r\n",
    "reluMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\r\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\r\n",
    "\r\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\r\n",
    "\r\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sigmoidMLP = Network()\r\n",
    "# Build MLP with FCLayer and SigmoidLayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "sigmoidMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reluMLP = Network()\r\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "reluMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\r\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "3f857618d8b6a3d270a8a45aea13fd0dc2a28a907e8d4ef70fe87ce2a92698f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}