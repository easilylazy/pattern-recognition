{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "tf.disable_eager_execution()\r\n",
    "\r\n",
    "from network import Network\r\n",
    "from solver import train, test\r\n",
    "from plot import plot_loss_and_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def decode_image(image):\r\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\r\n",
    "    image = tf.cast(image, tf.float32)\r\n",
    "    image = tf.reshape(image, [784])\r\n",
    "    image = image / 255.0\r\n",
    "    image = image - tf.reduce_mean(image)\r\n",
    "    return image\r\n",
    "\r\n",
    "def decode_label(label):\r\n",
    "    # Encode label with one-hot encoding\r\n",
    "    return tf.one_hot(label, depth=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Data Preprocessing\r\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\r\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\r\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\r\n",
    "\r\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\r\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\r\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "batch_size = 100\r\n",
    "max_epoch = 10\r\n",
    "init_std = 0.01\r\n",
    "\r\n",
    "learning_rate_SGD = 0.001\r\n",
    "weight_decay = 0.1\r\n",
    "\r\n",
    "hidden_layer1=128\r\n",
    "hidden_layer2=64\r\n",
    "disp_freq = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from criterion import EuclideanLossLayer\r\n",
    "from optimizer import SGD\r\n",
    "\r\n",
    "criterion = EuclideanLossLayer()\r\n",
    "\r\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\r\n",
    "\r\n",
    "### TODO\r\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from layers import FCLayer, SigmoidLayer\r\n",
    "\r\n",
    "sigmoidMLP = Network()\r\n",
    "# Build MLP with FCLayer and SigmoidLayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "sigmoidMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 26.5186\t Accuracy 0.1100\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 7.6214\t Accuracy 0.0957\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 4.5222\t Accuracy 0.0906\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 3.4299\t Accuracy 0.0928\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 2.8705\t Accuracy 0.0989\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 2.5267\t Accuracy 0.1047\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 2.2928\t Accuracy 0.1093\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 2.1247\t Accuracy 0.1135\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.9945\t Accuracy 0.1188\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.8923\t Accuracy 0.1238\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.8068\t Accuracy 0.1296\n",
      "\n",
      "Epoch [0]\t Average training loss 1.7367\t Average training accuracy 0.1356\n",
      "Epoch [0]\t Average validation loss 0.9962\t Average validation accuracy 0.2088\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.0120\t Accuracy 0.1900\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.9992\t Accuracy 0.2094\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.9886\t Accuracy 0.2134\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.9830\t Accuracy 0.2185\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.9749\t Accuracy 0.2281\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.9654\t Accuracy 0.2366\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.9568\t Accuracy 0.2443\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.9513\t Accuracy 0.2478\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.9443\t Accuracy 0.2545\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.9390\t Accuracy 0.2585\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.9322\t Accuracy 0.2653\n",
      "\n",
      "Epoch [1]\t Average training loss 0.9259\t Average training accuracy 0.2717\n",
      "Epoch [1]\t Average validation loss 0.8415\t Average validation accuracy 0.3546\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.8474\t Accuracy 0.3500\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.8490\t Accuracy 0.3510\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.3605\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.8406\t Accuracy 0.3594\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.8365\t Accuracy 0.3649\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.8307\t Accuracy 0.3715\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.8256\t Accuracy 0.3775\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.8231\t Accuracy 0.3786\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.8190\t Accuracy 0.3843\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.8165\t Accuracy 0.3871\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.8125\t Accuracy 0.3932\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8086\t Average training accuracy 0.3982\n",
      "Epoch [2]\t Average validation loss 0.7513\t Average validation accuracy 0.4712\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.7518\t Accuracy 0.4800\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.7611\t Accuracy 0.4608\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.7569\t Accuracy 0.4716\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.7570\t Accuracy 0.4668\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.7550\t Accuracy 0.4703\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.7511\t Accuracy 0.4761\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.7478\t Accuracy 0.4805\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.7470\t Accuracy 0.4813\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.7444\t Accuracy 0.4850\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.7432\t Accuracy 0.4873\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.7407\t Accuracy 0.4925\n",
      "\n",
      "Epoch [3]\t Average training loss 0.7382\t Average training accuracy 0.4960\n",
      "Epoch [3]\t Average validation loss 0.6952\t Average validation accuracy 0.5634\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.6928\t Accuracy 0.5800\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.7064\t Accuracy 0.5475\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.7035\t Accuracy 0.5523\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.7048\t Accuracy 0.5473\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.7040\t Accuracy 0.5490\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.7012\t Accuracy 0.5529\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.6990\t Accuracy 0.5560\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.6991\t Accuracy 0.5565\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.6973\t Accuracy 0.5594\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.6969\t Accuracy 0.5609\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.6954\t Accuracy 0.5649\n",
      "\n",
      "Epoch [4]\t Average training loss 0.6936\t Average training accuracy 0.5677\n",
      "Epoch [4]\t Average validation loss 0.6590\t Average validation accuracy 0.6244\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.6550\t Accuracy 0.6100\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.6711\t Accuracy 0.5980\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.6690\t Accuracy 0.6049\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.6711\t Accuracy 0.6002\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.6709\t Accuracy 0.6032\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.6689\t Accuracy 0.6072\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.6673\t Accuracy 0.6095\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.6680\t Accuracy 0.6091\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.6668\t Accuracy 0.6111\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.6668\t Accuracy 0.6118\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.6658\t Accuracy 0.6146\n",
      "\n",
      "Epoch [5]\t Average training loss 0.6646\t Average training accuracy 0.6167\n",
      "Epoch [5]\t Average validation loss 0.6352\t Average validation accuracy 0.6672\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.6304\t Accuracy 0.7200\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.6478\t Accuracy 0.6406\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.6464\t Accuracy 0.6463\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.6489\t Accuracy 0.6405\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.6492\t Accuracy 0.6420\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.6477\t Accuracy 0.6446\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.6465\t Accuracy 0.6459\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.6475\t Accuracy 0.6457\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.6467\t Accuracy 0.6473\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.6470\t Accuracy 0.6479\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.6464\t Accuracy 0.6498\n",
      "\n",
      "Epoch [6]\t Average training loss 0.6455\t Average training accuracy 0.6517\n",
      "Epoch [6]\t Average validation loss 0.6198\t Average validation accuracy 0.6998\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.6146\t Accuracy 0.7200\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.6326\t Accuracy 0.6690\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.6316\t Accuracy 0.6718\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.6344\t Accuracy 0.6665\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.6351\t Accuracy 0.6676\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.6338\t Accuracy 0.6698\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.6330\t Accuracy 0.6707\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.6342\t Accuracy 0.6703\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.6336\t Accuracy 0.6715\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.6342\t Accuracy 0.6722\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.6338\t Accuracy 0.6738\n",
      "\n",
      "Epoch [7]\t Average training loss 0.6331\t Average training accuracy 0.6754\n",
      "Epoch [7]\t Average validation loss 0.6101\t Average validation accuracy 0.7192\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.6048\t Accuracy 0.7500\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.6231\t Accuracy 0.6943\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.6223\t Accuracy 0.6943\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.6253\t Accuracy 0.6880\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.6262\t Accuracy 0.6882\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.6252\t Accuracy 0.6893\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.6245\t Accuracy 0.6897\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.6260\t Accuracy 0.6900\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.6256\t Accuracy 0.6906\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.6262\t Accuracy 0.6911\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.6261\t Accuracy 0.6923\n",
      "\n",
      "Epoch [8]\t Average training loss 0.6256\t Average training accuracy 0.6939\n",
      "Epoch [8]\t Average validation loss 0.6045\t Average validation accuracy 0.7372\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.5993\t Accuracy 0.7700\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.6175\t Accuracy 0.7116\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.6170\t Accuracy 0.7100\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.6201\t Accuracy 0.7037\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.6211\t Accuracy 0.7030\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.6203\t Accuracy 0.7042\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.6198\t Accuracy 0.7045\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.6214\t Accuracy 0.7043\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.6211\t Accuracy 0.7049\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.6219\t Accuracy 0.7051\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.6218\t Accuracy 0.7061\n",
      "\n",
      "Epoch [9]\t Average training loss 0.6215\t Average training accuracy 0.7074\n",
      "Epoch [9]\t Average validation loss 0.6021\t Average validation accuracy 0.7494\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7342.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\r\n",
    "\r\n",
    "### TODO\r\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from layers import ReLULayer\r\n",
    "\r\n",
    "reluMLP = Network()\r\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\r\n",
    "reluMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 9.0191\t Accuracy 0.0900\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 3.8776\t Accuracy 0.1212\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 2.9371\t Accuracy 0.1371\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 2.5121\t Accuracy 0.1533\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 2.2631\t Accuracy 0.1651\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 2.0906\t Accuracy 0.1771\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.9587\t Accuracy 0.1908\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.8578\t Accuracy 0.2017\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.7746\t Accuracy 0.2136\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.7042\t Accuracy 0.2237\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.6433\t Accuracy 0.2335\n",
      "\n",
      "Epoch [0]\t Average training loss 1.5917\t Average training accuracy 0.2432\n",
      "Epoch [0]\t Average validation loss 1.0281\t Average validation accuracy 0.3800\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.0507\t Accuracy 0.4500\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.0238\t Accuracy 0.3755\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.0075\t Accuracy 0.3795\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.9949\t Accuracy 0.3818\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.9850\t Accuracy 0.3858\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.9741\t Accuracy 0.3923\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.9632\t Accuracy 0.3975\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.9552\t Accuracy 0.4006\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.9465\t Accuracy 0.4052\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.9381\t Accuracy 0.4091\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.9295\t Accuracy 0.4136\n",
      "\n",
      "Epoch [1]\t Average training loss 0.9215\t Average training accuracy 0.4185\n",
      "Epoch [1]\t Average validation loss 0.8167\t Average validation accuracy 0.4986\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.8303\t Accuracy 0.5200\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.8205\t Accuracy 0.4904\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.8139\t Accuracy 0.4925\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.8104\t Accuracy 0.4926\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.8070\t Accuracy 0.4928\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.8022\t Accuracy 0.4970\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.7975\t Accuracy 0.4995\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.7950\t Accuracy 0.5003\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.7913\t Accuracy 0.5034\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.7878\t Accuracy 0.5056\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.7837\t Accuracy 0.5081\n",
      "\n",
      "Epoch [2]\t Average training loss 0.7798\t Average training accuracy 0.5112\n",
      "Epoch [2]\t Average validation loss 0.7198\t Average validation accuracy 0.5678\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.7290\t Accuracy 0.5700\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.7254\t Accuracy 0.5643\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.7222\t Accuracy 0.5636\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.7221\t Accuracy 0.5609\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.7207\t Accuracy 0.5598\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.7179\t Accuracy 0.5627\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.7151\t Accuracy 0.5635\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.7146\t Accuracy 0.5631\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.7126\t Accuracy 0.5647\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.7108\t Accuracy 0.5667\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.7085\t Accuracy 0.5676\n",
      "\n",
      "Epoch [3]\t Average training loss 0.7060\t Average training accuracy 0.5696\n",
      "Epoch [3]\t Average validation loss 0.6631\t Average validation accuracy 0.6196\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.6686\t Accuracy 0.6400\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.6698\t Accuracy 0.6102\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.6681\t Accuracy 0.6098\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.6696\t Accuracy 0.6090\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.6691\t Accuracy 0.6065\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.6672\t Accuracy 0.6082\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.6654\t Accuracy 0.6091\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.6658\t Accuracy 0.6075\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.6648\t Accuracy 0.6082\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.6638\t Accuracy 0.6096\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.6624\t Accuracy 0.6100\n",
      "\n",
      "Epoch [4]\t Average training loss 0.6607\t Average training accuracy 0.6111\n",
      "Epoch [4]\t Average validation loss 0.6263\t Average validation accuracy 0.6554\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.6274\t Accuracy 0.6500\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.6343\t Accuracy 0.6390\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.6333\t Accuracy 0.6392\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.6356\t Accuracy 0.6390\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.6355\t Accuracy 0.6371\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.6341\t Accuracy 0.6380\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.6328\t Accuracy 0.6386\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.6338\t Accuracy 0.6369\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.6333\t Accuracy 0.6373\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.6328\t Accuracy 0.6386\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.6319\t Accuracy 0.6388\n",
      "\n",
      "Epoch [5]\t Average training loss 0.6307\t Average training accuracy 0.6398\n",
      "Epoch [5]\t Average validation loss 0.6016\t Average validation accuracy 0.6870\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.5990\t Accuracy 0.6800\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.6105\t Accuracy 0.6637\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.6098\t Accuracy 0.6658\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.6126\t Accuracy 0.6621\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.6128\t Accuracy 0.6609\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.6117\t Accuracy 0.6611\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.6108\t Accuracy 0.6608\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.6122\t Accuracy 0.6591\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.6120\t Accuracy 0.6593\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.6118\t Accuracy 0.6610\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.6113\t Accuracy 0.6609\n",
      "\n",
      "Epoch [6]\t Average training loss 0.6103\t Average training accuracy 0.6615\n",
      "Epoch [6]\t Average validation loss 0.5847\t Average validation accuracy 0.7080\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.5789\t Accuracy 0.7000\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.5943\t Accuracy 0.6824\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.5938\t Accuracy 0.6853\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.5968\t Accuracy 0.6805\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.5973\t Accuracy 0.6797\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.5964\t Accuracy 0.6791\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.5957\t Accuracy 0.6790\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.5974\t Accuracy 0.6772\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.5974\t Accuracy 0.6769\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.5974\t Accuracy 0.6783\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.5972\t Accuracy 0.6778\n",
      "\n",
      "Epoch [7]\t Average training loss 0.5964\t Average training accuracy 0.6781\n",
      "Epoch [7]\t Average validation loss 0.5731\t Average validation accuracy 0.7240\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.5652\t Accuracy 0.7200\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.5831\t Accuracy 0.6951\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.5828\t Accuracy 0.6987\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.5860\t Accuracy 0.6939\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.5867\t Accuracy 0.6929\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.5859\t Accuracy 0.6913\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.5855\t Accuracy 0.6915\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.5873\t Accuracy 0.6905\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.5874\t Accuracy 0.6903\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.5876\t Accuracy 0.6917\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.5875\t Accuracy 0.6910\n",
      "\n",
      "Epoch [8]\t Average training loss 0.5869\t Average training accuracy 0.6915\n",
      "Epoch [8]\t Average validation loss 0.5654\t Average validation accuracy 0.7370\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.5559\t Accuracy 0.7300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.5757\t Accuracy 0.7059\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.5754\t Accuracy 0.7095\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.5788\t Accuracy 0.7034\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.5796\t Accuracy 0.7027\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.5789\t Accuracy 0.7016\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.5786\t Accuracy 0.7019\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.5805\t Accuracy 0.7008\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.5808\t Accuracy 0.7002\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.5811\t Accuracy 0.7017\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.5811\t Accuracy 0.7009\n",
      "\n",
      "Epoch [9]\t Average training loss 0.5806\t Average training accuracy 0.7011\n",
      "Epoch [9]\t Average validation loss 0.5605\t Average validation accuracy 0.7430\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7200.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\r\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\r\n",
    "\r\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\r\n",
    "\r\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "sigmoidMLP = Network()\r\n",
    "# Build MLP with FCLayer and SigmoidLayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "sigmoidMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "sigmoidMLP.add(SigmoidLayer())\r\n",
    "sigmoidMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 0.8167\t Accuracy 0.1400\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.8018\t Accuracy 0.1125\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.7862\t Accuracy 0.1127\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.7708\t Accuracy 0.1128\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.7544\t Accuracy 0.1142\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.7376\t Accuracy 0.1140\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.7230\t Accuracy 0.1140\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.7072\t Accuracy 0.1143\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.6926\t Accuracy 0.1141\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.6793\t Accuracy 0.1140\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.6659\t Accuracy 0.1136\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6537\t Average training accuracy 0.1129\n",
      "Epoch [0]\t Average validation loss 0.5183\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.5050\t Accuracy 0.1400\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.5026\t Accuracy 0.1125\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.4950\t Accuracy 0.1127\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.4874\t Accuracy 0.1128\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.4792\t Accuracy 0.1142\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.4708\t Accuracy 0.1140\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.4635\t Accuracy 0.1140\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.4555\t Accuracy 0.1143\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.4481\t Accuracy 0.1141\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.4414\t Accuracy 0.1140\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4346\t Accuracy 0.1136\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4284\t Average training accuracy 0.1129\n",
      "Epoch [1]\t Average validation loss 0.3594\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.3500\t Accuracy 0.1400\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.3509\t Accuracy 0.1125\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.3470\t Accuracy 0.1127\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.3431\t Accuracy 0.1128\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.3389\t Accuracy 0.1142\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.3345\t Accuracy 0.1140\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.3307\t Accuracy 0.1140\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.3264\t Accuracy 0.1143\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3224\t Accuracy 0.1141\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.3188\t Accuracy 0.1140\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.3152\t Accuracy 0.1136\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3118\t Average training accuracy 0.1129\n",
      "Epoch [2]\t Average validation loss 0.2745\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.2682\t Accuracy 0.1400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.2694\t Accuracy 0.1125\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2674\t Accuracy 0.1127\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.2652\t Accuracy 0.1128\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.2628\t Accuracy 0.1142\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.2603\t Accuracy 0.1140\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2581\t Accuracy 0.1140\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.2556\t Accuracy 0.1143\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2533\t Accuracy 0.1141\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.2512\t Accuracy 0.1140\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.2491\t Accuracy 0.1136\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2471\t Average training accuracy 0.1129\n",
      "Epoch [3]\t Average validation loss 0.2251\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.2207\t Accuracy 0.1400\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.2218\t Accuracy 0.1125\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.2206\t Accuracy 0.1127\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.2193\t Accuracy 0.1128\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.2178\t Accuracy 0.1142\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.2162\t Accuracy 0.1140\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2148\t Accuracy 0.1140\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2133\t Accuracy 0.1143\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.2118\t Accuracy 0.1141\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2105\t Accuracy 0.1140\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.2091\t Accuracy 0.1136\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2079\t Average training accuracy 0.1129\n",
      "Epoch [4]\t Average validation loss 0.1939\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.1907\t Accuracy 0.1400\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.1916\t Accuracy 0.1125\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.1908\t Accuracy 0.1127\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.1900\t Accuracy 0.1128\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.1890\t Accuracy 0.1142\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.1879\t Accuracy 0.1140\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.1870\t Accuracy 0.1140\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.1860\t Accuracy 0.1143\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.1850\t Accuracy 0.1141\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.1841\t Accuracy 0.1140\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.1832\t Accuracy 0.1136\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1823\t Average training accuracy 0.1129\n",
      "Epoch [5]\t Average validation loss 0.1728\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.1704\t Accuracy 0.1400\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.1711\t Accuracy 0.1125\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.1706\t Accuracy 0.1127\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.1700\t Accuracy 0.1128\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.1693\t Accuracy 0.1142\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.1686\t Accuracy 0.1140\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.1680\t Accuracy 0.1140\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.1672\t Accuracy 0.1143\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.1665\t Accuracy 0.1141\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.1659\t Accuracy 0.1140\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.1652\t Accuracy 0.1136\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1646\t Average training accuracy 0.1129\n",
      "Epoch [6]\t Average validation loss 0.1579\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.1559\t Accuracy 0.1400\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.1566\t Accuracy 0.1125\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.1562\t Accuracy 0.1127\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.1558\t Accuracy 0.1128\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.1553\t Accuracy 0.1142\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.1548\t Accuracy 0.1140\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.1543\t Accuracy 0.1140\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.1538\t Accuracy 0.1143\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.1532\t Accuracy 0.1141\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.1528\t Accuracy 0.1140\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.1523\t Accuracy 0.1136\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1519\t Average training accuracy 0.1129\n",
      "Epoch [7]\t Average validation loss 0.1469\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.1453\t Accuracy 0.1400\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.1458\t Accuracy 0.1125\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.1456\t Accuracy 0.1127\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.1453\t Accuracy 0.1128\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.1449\t Accuracy 0.1142\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.1445\t Accuracy 0.1140\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.1442\t Accuracy 0.1140\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.1437\t Accuracy 0.1143\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.1433\t Accuracy 0.1141\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.1430\t Accuracy 0.1140\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.1426\t Accuracy 0.1136\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1423\t Average training accuracy 0.1129\n",
      "Epoch [8]\t Average validation loss 0.1385\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.1371\t Accuracy 0.1400\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.1377\t Accuracy 0.1125\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.1375\t Accuracy 0.1127\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.1373\t Accuracy 0.1128\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.1370\t Accuracy 0.1142\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.1367\t Accuracy 0.1140\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.1364\t Accuracy 0.1140\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.1361\t Accuracy 0.1143\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.1358\t Accuracy 0.1141\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.1355\t Accuracy 0.1140\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.1352\t Accuracy 0.1136\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1350\t Average training accuracy 0.1129\n",
      "Epoch [9]\t Average validation loss 0.1320\t Average validation accuracy 0.1060\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.1135.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\r\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "reluMLP = Network()\r\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\r\n",
    "# 128 is the number of hidden units, you can change by your own\r\n",
    "reluMLP.add(FCLayer(784, hidden_layer1))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer1, hidden_layer2))\r\n",
    "reluMLP.add(ReLULayer())\r\n",
    "reluMLP.add(FCLayer(hidden_layer2, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "bad allocation",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6340/1585392641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreluMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreluMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisp_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\learningMaterials\\pattern-recognition\\homework-12345\\homework-2\\homework2-mlp\\solver.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, dataset, max_epoch, batch_size, disp_freq)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;31m# Training process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \t\t\tbatch_train_loss, batch_train_acc = train_one_epoch(model, criterion, optimizer, train_get_next,\n\u001b[0m\u001b[0;32m     36\u001b[0m \t\t\t                                                    max_epoch, batch_size, disp_freq, epoch, sess)\n\u001b[0;32m     37\u001b[0m                         \u001b[0mbatch_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_val_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_get_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\learningMaterials\\pattern-recognition\\homework-12345\\homework-2\\homework2-mlp\\solver.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, criterion, optimizer, data_get_next, max_epoch, batch_size, disp_freq, epoch, sess)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_train_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;31m# Get training data and label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_get_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32mD:\\programx64\\Anaconda3\\envs\\tf_latest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: bad allocation"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\r\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf_latest': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "cfc659ec162c9312116a4448dc35362c57c7c097b4e6d43fc665f911a164248d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}